Type: original
Title: Amateurs Talk Detection. Professionals Talk Collection.
Link: collection-and-detection.html
Pubdate: 2022/05/14 10:24:22
Category: Cybersecurity
Author: Zac Szewczyk

I went to a presentation from a large cybersecurity firm the other day. The salesmen--and they were all salesmen, as even a few post-presentation questions made clear--focused on the intelligence their company produced, but knew little about the rest of their company's [intelligence cycle](https://www.jcs.mil/Doctrine/Joint-Doctrine-Pubs/2-0-Intelligence-Series/). As a consumer of their products, though, I consider knowledge of the process that created them critical. After an hour on finished intelligence, and in particular a slide that touted a "globally deployed sensor grid", I wanted to know more about that production process in general, and about their collection specifically. Unfortunately, when asked, the salesmen offered a handful of handwavy "We have millions of sensors across the globe", and, "We ingest billions of events per day" statements, but little beyond that.

I left the venue disappointed but not surprised. Whether dealing with cyber threat intelligence specifically, intelligence in general, or even the types of analysis SOC personnel perform, almost no one wants to talk about the *process*--they just want to hear about the *results*. "APT X used Malware Y." "The SOC identified a compromise in Network Z." This myopic focus on the assessments themselves precludes an understanding of the entire process which would, critically, involve evaluating the impact of biases at each stage of the intelligence cycle on the end result. For example, how did guidance given during **Planning and Direction** inform **Collection**? Did limited or directed collection--a statistical bias[^1]--exacerbate human or systemic biases during **Processing and Exploitation** or **Analysis and Production**? How did decisions at each phase ultimately impact the accuracy of the assessment delivered in **Dissemination and Integration**? Answers to questions like these provide the necessary background to appropriately contextualize finished intelligence products, which are otherwise the result of an opaque process and subject to unclear biases at each stage.

This is not a novel insight. As Andrew Thompson frequently [quips](https://twitter.com/ImposeCost/status/1521965886042689542), "If your collection is trash, your analysis will be trash." That holds for intelligence in general, and cyber threat intelligence specifically. Exquisite analysis can never overcome incomplete data. Over the last few days, I realized that rule also applies to the type of analysis SOC personnel perform as well.[^2] Effective analysis, whether in the intelligence or information security field, requires a transparent and rigorous process--but it *relies* on collection. Thanks to the fantastic work [threat intelligence companies and individual researchers publish](Threat Intel and Threat Research.txt), and the myriad rule repositories available to SOC analysts[^3], the challenge in uncovering malicious activity is almost never in its actual identification; that is--for the most part--straightforward. The challenge in uncovering malicious activity is almost always in collecting the data necessary to enable its detection. "If your collection is trash, your analysis will be trash", and that applies to the information security field as well.

General Omar Bradley once said, "Amateurs talk strategy. Professionals talk logistics." This does not devalue strategy, but rather highlights the importance of logistics as an enabler of strategic plans. I propose a similar statement for the information security space not to devalue detection, but rather to highlight the importance of the collection upon which it relies: "Amateurs talk detection. Professionals talk collection."

[>1] NIST Special Publication 1270: [*Proposal for Identifying and Managing Bias in Artificial Intelligence*](https://www.nist.gov/artificial-intelligence/proposal-identifying-and-managing-bias-artificial-intelligence-sp-1270) does a nice job of identifying sources of bias. Whereas most think of biases as logical fallacies, the authors of SP 1270 correctly identify three distinct sources: statistical biases "such as representativeness of datasets and fairness of machine learning algorithms", human biases, and systemic biases. 

[>2] Again, as I said in [*SOC Metrics, Part I: Foundational Metrics*](https://zacs.site/blog/soc-metrics-part-i.html), "I use 'SOC' as a generic term for all groups responsible for securing an organization's information systems. This includes the IT administrators who provision and maintain those systems, security and compliance monitors who deal with known threats and policy violations, and threat hunters who deal with emerging and novel threats. Each of these entities plays a distinct but critical role in an effective security program, but I will use 'SOC' as a general term for all of them here."

[>3] For instance, start with MITRE's [Cyber Analytics Repository](https://car.mitre.org/) (CAR) or the famous [Sigma](https://github.com/SigmaHQ/sigma) project. Also check out rules from [Azure Sentinel](https://github.com/Azure/Azure-Sentinel), [Google Chronicle](https://github.com/chronicle/detection-rules), [DNIF](https://github.com/dnif/content), [Splunk](https://research.splunk.com/detections/), [Elastic](https://github.com/elastic/detection-rules/), [FalconForce](https://github.com/FalconForceTeam/FalconFriday), [Panther Labs](https://github.com/panther-labs/panther-analysis), [Emerging Threats](https://rules.emergingthreats.net/), and [SOC Prime](https://socprime.com/). Don't forget about [advanced hunting queries for Microsoft 365 Defender](https://github.com/microsoft/Microsoft-365-Defender-Hunting-Queries), Elastic's [Prebuilt Rules](https://www.elastic.co/guide/en/security/current/prebuilt-rules.html), Google's [Community Security Analytics](https://github.com/GoogleCloudPlatform/security-analytics) (CSA), and Open Threat Research Forge's [ThreatHunter Playbook](https://github.com/OTRF/ThreatHunter-Playbook), too.