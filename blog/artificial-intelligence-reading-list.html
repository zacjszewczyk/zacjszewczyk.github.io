<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="author" content="Zac Szewczyk">
        <meta name="application-name" content="Zac Szewczyk">
        <meta name="keywords" content="Zac Szewczyk, Zachary Szewczyk, Zac J. Szewczyk, Szewczyk, zacjszewczyk">
        <meta name="description" content="Thanks in no small part to OpenAI's [ChatGPT](https://chat.openai.com), the past year has seen an explosion in interest in artificial intelligence in general, and in large language models in particular. That democratization of access turned this niche research area into a common topic of conversation, and has led to a lot of fascinating writing on the subject. Although certainly not comprehensive, this article collects some of my favorite articles, papers, and resources into a single reading list.">
        
        <meta name="robots" content="index, follow">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover"> 
        <meta name="referrer" content="no-referrer">
        <meta name="theme-color" content="#FFFFFF">
        <link rel="stylesheet" href="/assets/main.css">
        
        <meta property="og:image" content="/assets/images/favicon.ico">
        <meta property="og:title" content="Artificial Intelligence Reading List - Zac Szewczyk">
        <meta property="og:site_name" content="Zac Szewczyk's Blog">
        <meta property="og:article:author" content="Zac Szewczyk">
        <meta property="og:see_also" content="https://zacs.site/">

        <link rel="alternate" type="application/rss+xml" title="Zac Szewczyk's Feed" href="/rss.xml">
        <link rel="shortcut icon" type="image/ico" size="16x16" href="/assets/images/favicon.ico">
        <link rel="shortcut icon" type="image/ico" size="192x192" href="/assets/images/favicon_192.ico">
        <link rel="shortcut icon" type="image/ico" size="512x512" href="/assets/images/favicon_512.ico">
        
        <title>Artificial Intelligence Reading List - Zachary Szewczyk</title>
    </head>
    <body id="{{BODYID}}">
        <nav>
            <a href="/index.html" id="home_link">Home</a>
            <a href="/blog.html" id="blog_link">Blog</a>
            <a href="/explore.html" id="explore_link">Explore</a>
            <a href="/rss.xml" id="rss_link">RSS</a>
            <a href="/archives.html" id="archives_link" rel="prefetch">Post Archives</a>
            <a href="/projects.html" id="projects_link">Projects</a>
            <a href="/disclaimers.html" id="disclaimers_link">Disclaimers</a>
        </nav>
        <main>
<article>
<h2 id='article_title'>
<a class="original" href="/blog/artificial-intelligence-reading-list.html">Artificial Intelligence Reading List</a>
</h2>
<time id='article_time' datetime="2023-12-08 22:03:26-0400" pubdate="pubdate">By <link rel="author">Zac Szewczyk</link> on <a href="/blog/2023.html">2023</a>/<a href="/blog/2023-12.html">12</a>/08 22:03:26 EST in <a href='/tech.html'>Tech</a></time>
<p>Thanks in no small part to OpenAI&#8217;s <a href="https://chat.openai.com">ChatGPT</a>, the past year has seen an explosion in interest in artificial intelligence in general, and in large language models in particular. That democratization of access turned this niche research area into a common topic of conversation, and has led to a lot of fascinating writing on the subject. Although certainly not comprehensive, this article collects some of my favorite articles, papers, and resources into a single reading list.</p>

<p>For more on this subject, check out Vicky Boykis&#8217; <a href="https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e"><em>Anti-hype LLM reading list</em></a> which features some of the same artificial intelligence-related resources. Vikas Gorur also posted an informative article, <a href="https://gorur.dev/posts/path-to-llms.html"><em>Path to LLMs</em></a>, that contains helpful resources for everything from foundational mathematics to building modern large language models.</p>

<p>I have organized this reading list into several sections loosely by focus and generally ordered by publication date. <a href="#introduction-to-artificial-intelligence">Introduction to Artificial Intelligence</a> contains a high-level primer on the field. <a href="#large-language-model-fundamentals">Large Language Model Fundamentals</a> delves into some lower-level details but should still be approachable for most laypeople. <a href="#artificial-intelligence-theory">Artificial Intelligence Theory</a> deals with interesting theoretical questions like, &#8220;What really counts as intelligence?&#8221; <a href="#applications-for-artificial-intelligence">Applications for Artificial Intelligence</a> contains some interesting reading on the possible applications&#160;&#8212;&#160;and limitations&#160;&#8212;&#160;of artificial intelligence in general settings, while <a href="#military-applications-for-artificial-intelligence">Military Applications for Artificial Intelligence</a> focuses on the same topic but in military settings. Finally, <a href="#additional-resources">Additional Resources</a> lists a few useful tools and resources not specifically related to the other sections. I added notes to some of these entries but not all of them.</p>

<h3 class="headers" id="IntroductiontoArtificialIntelligence">Introduction to Artificial Intelligence<span>&nbsp;<a href="#IntroductiontoArtificialIntelligence">#</a></span></h3>

<p>This section is a high-level primer on the field of artificial intelligence.</p>

<ul>
    <li><a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html"><em>The AI Revolution: The Road to Superintelligence</em></a> and <a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html"><em>The AI Revolution: Our Immortality or Extinction</em></a>. Back in 2015, Tim Urban wrote a lengthy yet approachable two-part series speculating on the future impact of artificial super intelligence.</li>
<li><a href="https://writings.stephenwolfram.com/2023/10/how-to-think-computationally-about-ai-the-universe-and-everything/"><em>How to Think Computationally about AI, the Universe and Everything</em></a>. Stephen Wolfram&#8217;s TED AI talk discusses the central role of computation in AI and the universe. He theorizes that the universe is composed of discrete computational elements and introduces the &#8220;ruliad&#8221;, a computational universe explored by AI. He also emphasizes the importance of computational language in bridging the gap between human understanding and computational reality. He also talks about the ruliad in <a href="https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/"><em>Generative AI Space and the Mental Imagery of Alien Minds</em></a></li>
<li><a href="https://blog.samaltman.com/ai"><em>AI</em></a>. Ten years ago, Sam Altman described what I think is the current best use of artificial intelligence: &#8220;The most positive outcome I can think of is one where computers get really good at doing, and humans get really good at thinking.&#8221;</li>
</ul>

<h3 class="headers" id="LargeLanguageModelFundamentals">Large Language Model Fundamentals<span>&nbsp;<a href="#LargeLanguageModelFundamentals">#</a></span></h3>

<p>This section delves into some lower-level details of a particular form of artificial intelligence, large language models, but should still be approachable for most laypeople.</p>

<ul>
    <li><a href="https://arxiv.org/abs/2501.09223"><em>Foundations of Large Language Models</em></a>. Tong Xiao and Jingbo Zhu cover foundational concepts of large language models.</li>
<li><a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/"><em>What is ChatGPT doing and why does it work?</em></a>. Stephen Wolfram explains in detail how large language models work.</li>
<li><a href="https://towardsdatascience.com/all-languages-are-not-created-tokenized-equal-cd87694a97c1"><em>All Languages Are NOT Created (Tokenized) Equal</em></a>. Yennie Jun explores the impact of English-centric training in large language models. Vox also made a good video on this subject: <a href="https://youtu.be/a2DgdsE86ts"><em>Why AI doesn&#8217;t speak every language</em></a>. In a similar vein, <a href="https://www.noemamag.com/the-babelian-tower-of-ai-alignment/"><em>The Babelian Tower Of AI Alignment</em></a> discusses a related issue of cultural biases affecting AI.</li>
<li><a href="https://blog.j11y.io/2023-11-22_multifaceted/"><em>Multifaceted: the linguistic echo chambers of LLMs</em></a>. In a similar vein, James Padolsey explores the root cause of curious linguistic tendencies in large language models. As artificial intelligence systems generate more internet content, this will become more and more pronounced as successive generations exacerbate the biases of their predecessors.</li>
<li><a href="https://blog.briankitano.com/llama-from-scratch/"><em>Llama from scratch</em></a>. Brian Kitano walks through his own implementation of Meta&#8217;s LLaMA.</li>
<li><a href="https://arxiv.org/abs/2303.18223"><em>A Survey of Large Language Models</em></a>. This fantastic paper touches on every aspect of large language models, from their history to the underlying theory to the performance today.</li>
<li><a href="https://magazine.sebastianraschka.com/p/understanding-large-language-models"><em>Understanding Large Language Models</em></a>. Sebastian Raschka presents a concise explanation, and a curated list of resources, for understanding large language models.</li>
<li>Anthropic’s <a href="https://www.anthropic.com/research/mapping-mind-language-model"><em>Mapping the Mind of a Large Language Model</em></a> used sparse autoencoders to chart millions of human-readable features inside Claude, demonstrating that coherent concepts are geometrically localized. The companion Transformer Circuits studies, <a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html"><em>Circuit Tracing: Revealing Computational Graphs in Language Models</em></a> and <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html"><em>On the Biology of a Large Language Model</em></a>, introduced attribution graphs that expose token-level credit assignment and showed these graphs organize into reusable, function-specific circuits reminiscent of biological modularity. Finally, Anthropic’s <a href="https://www.anthropic.com/research/tracing-thoughts-language-model"><em>Tracing the Thoughts of a Large Language Model</em></a> linked the concept atlas with circuit tracing to follow causal chains over time, pushing LLM interpretability toward auditable, mechanistic explanations of reasoning versus their present black box nature. Similar research to improve the interpretability of reasoning models has met with <a href="https://www.anthropic.com/research/reasoning-models-dont-say-think">mixed results</a>.</li>
</ul>

<h3 class="headers" id="ArtificialIntelligenceTheory">Artificial Intelligence Theory<span>&nbsp;<a href="#ArtificialIntelligenceTheory">#</a></span></h3>

<p>This section deals with interesting theoretical questions like, &#8220;What really counts as intelligence?&#8221;</p>

<ul>
    <li><a href="https://writings.stephenwolfram.com/2022/06/alien-intelligence-and-the-concept-of-technology/"><em>Alien Intelligence and the Concept of Technology</em></a>. Stephen Wolfram explores the idea that all processes are fundamentally equivalent in computational terms. He suggests that what we consider intelligence, governed by physics, may not be fundamentally different from &#8220;alien&#8221; processes, challenging traditional views of intelligence and technology.</li>
<li><a href="https://www.noemamag.com/artificial-general-intelligence-is-already-here/"><em>Artificial General Intelligence is Already Here</em></a>. &#8220;Today&#8217;s most advanced AI models have many flaws, but decades from now, they will be recognized as the first true examples of artificial general intelligence.&#8221;</li>
<li><a href="https://moultano.wordpress.com/2023/06/28/the-many-ways-that-digital-minds-can-know/"><em>The Many Ways that Digital Minds can Know</em></a>. In the theme of, &#8220;What really counts as intelligence?&#8221;, Ryan Moulton shares some relevant thoughts. Michael Levin also explores this question in <a href="https://www.noemamag.com/ai-could-be-a-bridge-toward-diverse-intelligence/"><em>The Space Of Possible Minds</em></a>.</li>
<li><a href="https://www.lesswrong.com/posts/HxRjHq3QG8vcYy4yy/the-stochastic-parrot-hypothesis-is-debatable-for-the-last"><em>The Stochastic Parrot Hypothesis</em></a>. Quentin Feuillade-Montixi and Pierre Peigne evaluate GPT4&#8217;s performance against the stochastic parrot hypothesis, challenging the idea that it is &#8220;only&#8221; regurgitating words.</li>
<li><a href="https://www.noemamag.com/feral-intelligence/"><em>Are Large Language Models Conscious?</em></a>. Sebastian Konig discusses the role that language plays in determining consciousness in an interesting exploration of the question, “Are large language models more than ‘just’ machines?”</li>
<li><a href="https://arxiv.org/abs/2303.12712"><em>Sparks of Artificial General Intelligence: Early Experiments with GPT-4</em></a>. This controversial paper from 2023 stops short of declaring GPT-4 an instance of artificial general intelligence, but it does offer some compelling arguments that the model&#8217;s emergent abilities indicate it is more than <em>just</em> an autocomplete engine or math function.</li>
<li><a href="https://arxiv.org/abs/2304.15004"><em>Are Emergent Abilities of Large Language Models a Mirage?</em></a>. Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo examine some emergent properties of large language models and offer explanations for them.</li>
<li><a href="https://www.lesswrong.com/posts/CkhJAxHeyFCg2EcET/are-language-models-good-at-making-predictions"><em>Are Language Models Good at Making Predictions?</em></a>. An evaluation of large language models&#8217; ability to predict outcomes. In <a href="https://www.thealgorithmicbridge.com/p/harvard-and-mit-study-ai-models-are"><em>Harvard and MIT Study: AI Models Are Not Ready to Make Scientific Discoveries</em></a>, Alberto Romero walks through research from MIT and Harvard that indicates large language models predict but do not build an internal model of the world upon which to generalize their predictions.</li>
<li><a href="https://tiplur-bilrex.tlon.network/applied-fallibilism"><em>Applied Fallabilism: A Design Concept for Superintelligent Machines</em></a>. In <a href="https://tiplur-bilrex.tlon.network/applied-fallibilism/part-1-two-categories-of-knowledge">part one</a>, the author argues that “induction constrains and cannot support deduction”, that deduction is necessary to achieve artificial general intelligence, and describes how it may be achieved. <a href="https://tiplur-bilrex.tlon.network/applied-fallibilism/part-2-design-principles-for-an-explanatory-world-model-and-for-agi">Part two</a> explained design principles for building that world model. <a href="https://tiplur-bilrex.tlon.network/applied-fallibilism/part-3-comparison-with-present-ai-methods">Part 3</a> dealt with the apparent emergent properties of current models and promising avenues for achieving an explanatory world model. <a href="https://tiplur-bilrex.tlon.network/applied-fallibilism/part-4-predictions">Part 4</a> contained some predictions for what it would take to achieve artificial general intelligence and what that might look like. <a href="https://tiplur-bilrex.tlon.network/applied-fallibilism/part-5-a-toy-example—automating-construction-of-an—explanatory-knowledge-structure-using-an-llm W">Part 5</a> walks through an example of what this process might look like at a high level. While dense, this series is informative.</li>
<li><a href="https://arxiv.org/pdf/2311.02462.pdf"><em>Levels of AGI: Operationalizing Progress on the Path to AGI</em></a>. From Google’s DeepMind team, this paper “proposes a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors.”</li>
<li><a href="https://www.oneusefulthing.org/p/google-gemini-advanced-tasting-notes"><em>Google’s Gemini Advanced: Tasting Notes and Implications</em></a>. Under the guise of reviewing Google’s latest model, Gemini Advanced, Ethan Mollick shared some insightful observations on the state of large language models with an eye toward the future. I think the idea of ghosts is fascinating: “[What many have called ‘sentience’] is the illusion of a person on the other end of the line, even though there is nobody there. GPT-4 is full of ghosts. Gemini is also full of ghosts.”</li>
<li><a href="https://www.anthropic.com/research/claude-character"><em>Claude&#8217;s Character</em></a>. Anthropic, one of OpenAI&#8217;s primary competitors, talks about how the company imbues <em>character</em>&#160;&#8212;&#160;what some might call <em>personality</em>&#160;&#8212;&#160;within its flagship model, Claude. Experiments like these further blur the lines between machine and human, making that a more academic than practical debate.</li>
<li><a href="https://arcprize.org/blog/oai-o3-pub-breakthrough"><em>OpenAI o3 Breakthrough High Score on ARC-AGI</em></a>. While the specific observations on which this article is based are likely to become outdated soon, Francois Chollet&#8217;s thoughts on machine performance versus human intelligence in the context of artificial general intelligence are far more evergreen.</li>
<li><a href="https://dstrohmaier.com/better-ai-criticism/"><em>Suggestions for Better AI Criticism</em></a>. Many continue to push outdated or wholly inaccurate critiques of AI. David Strohmaier offers some helpful advice for making those critiques productive. Harry Law made similar points in <a href="https://www.learningfromexamples.com/p/what-academics-get-wrong"><em>Academics are kidding themselves about AI</em></a>, which addresses several common critiques of large language models and offers helpful advice for keeping up with the field. See also <a href="https://www.learningfromexamples.com/p/does-ai-know-things"><em>Yes, linear algebra can &#8216;know&#8217; things</em></a> where he tackles the question of whether or not models actually <em>know</em> things, one of many problematic imprecisions in the discourse.</li>
</ul>

<h3 class="headers" id="ApplicationsforArtificialIntelligence">Applications for Artificial Intelligence<span>&nbsp;<a href="#ApplicationsforArtificialIntelligence">#</a></span></h3>

<p>This section contains some reading on applications&#160;&#8212;&#160;and limitations&#160;&#8212;&#160;of artificial intelligence.</p>

<ul>
    <li><a href="https://huyenchip.com/2025/01/07/agents.html"><em>Agents</em></a>. Chip Huyen adapted a chapter of his book <a href="https://amzn.to/49j1cGS"><em>AI Engineering</em></a> that delves deeply into the theory and possible structure of effective artificial intelligence agents. Many of these ideas fit well with Microsoft&#8217;s observations in <a href="https://arxiv.org/abs/2303.12712"><em>Sparks of Artificial General Intelligence: Early Experiments with GPT-4</em></a>, when the authors theorized about the future potential of GPT-4-like models to act on their own. I found Chip&#8217;s discussion of the ability of large language models to plan particularly interesting. Also check out Anthropic&#8217;s article, <a href="https://www.anthropic.com/research/building-effective-agents"><em>Building effective agents</em></a>. In <a href="https://www.oneusefulthing.org/p/the-bitter-lesson-versus-the-garbage"><em>The Bitter Lesson versus The Garbage Can</em></a>, Ethan Mollick makes the interesting case that to be successful, agents won&#8217;t need to understand specific processes to optimize them.</li>
<li><a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms"><em>Understanding Reasoning LLMs</em></a>. Shortly after DeepSeek R1 was released, Sebastian Raschka wrote a great article explaining where reasoning models excel, where they fall short, and how to build them. Also check out the article in series on reasoning models, <a href="https://magazine.sebastianraschka.com/p/state-of-llm-reasoning-and-inference-scaling"><em>The State of LLM Reasoning Models</em></a>.</li>
<li><a href="https://arxiv.org/abs/2211.01910"><em>Large Language Models Are Human-Level Prompt Engineers</em></a>. This paper introduces Automatic Prompt Engineer, an algorithm that uses large language models to generate prompts and then uses other large language models to evaluate those prompts to select the best one. APE treats prompts as programs and optimizes them by searching over a pool of candidates proposed, aiming to maximize a chosen score function. Experiments demonstrate that APE&#8217;s automatically generated instructions outperform prior LLM baselines and achieve performance comparable to human-generated prompts.</li>
<li><a href="https://arxiv.org/abs/2502.01839"><em>Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification</em></a>. Traditionally, scaling the performance of large language models (LLMs) has been achieved through two main methods: increasing model size and scaling training data. This paper introduces a third approach: scaling test-time computation via sampling-based search. This method involves generating multiple candidate responses during inference and selecting the best one through self-verification in a similar manner as <em>Large Language Models Are Human-Level Prompt Engineers</em>. By scaling up this sampling-based search, the authors demonstrate significant improvements in reasoning capabilities.</li>
<li><a href="https://ezyang.github.io/ai-blindspots/"><em>AI Blindspots</em></a>. Just as important as understanding the power of artificial intelligence is understanding where it falls short. Although these examples are programming-specific, the general lessons are applicable for more broadly.</li>
<li><a href="https://arxiv.org/abs/2306.05685"><em>Judging LLM‑as‑a‑Judge with MT‑Bench and Chatbot Arena</em></a>. This study explores using powerful LLMs as judges to assess other LLM-based chat assistants. The authors show LLM judges agree with human preferences over 80% of the time&#160;&#8212;&#160;comparable to human-human agreement&#160;&#8212;&#160;supporting their use as scalable, explainable proxies for human evaluation.</li>
<li><a href="https://softwaredoug.com/blog/2025/01/21/llm-judge-decision-tree"><em>Classic ML to cope with Dumb LLM Judges</em></a>. Doug Turnbull describes how he enhanced search relevance by combining outputs from &#8220;dumb&#8221; LLM-based pairwise judges via a decision tree. By collecting simple LLM judgments (e.g., left/right/neither on product relevance attributes) and using these as features, the decision-tree ensemble notably improved precision.</li>
<li><a href="https://codinginterviewsmadesimple.substack.com/p/fine-tuning-llms-is-a-huge-waste"><em>Fine‑Tuning LLMs is a Huge Waste of Time</em></a>. Devansh argues that fine-tuning LLMs for knowledge injection is often overrated, suggesting that alternative methods&#160;&#8212;&#160;like retrieval-augmented generation and prompt engineering&#160;&#8212;&#160;are more effective and cost-efficient. <a href="https://artificialintelligencemadesimple.substack.com/p/a-new-way-to-control-language-model"><em>A New Way to Control Language Model Generations</em></a> explores some of those alternative ways. </li>
</ul>

<h3 class="headers" id="MilitaryApplicationsforArtificialIntelligence">Military Applications for Artificial Intelligence<span>&nbsp;<a href="#MilitaryApplicationsforArtificialIntelligence">#</a></span></h3>

<p>This section contains some reading on the possible applications&#160;&#8212;&#160;and limitations&#160;&#8212;&#160;of artificial intelligence in military settings.</p>

<ul>
    <li><a href="https://mwi.westpoint.edu/laplaces-demon-and-the-black-box-of-artificial-intelligence/"><em>Laplace&#8217;s Demon and the Black Box of Artificial Intelligence</em></a>. Thom Hawkins explores some of the challenges of relying on artificial intelligence in a military context. See also: <a href="https://zacs.site/blog/you-need-an-algorithm.html"><em>You Don&#8217;t Need AI, You Need an Algorithm</em></a>.</li>
<li><a href="https://www.lawfaremedia.org/article/what-chatgpt-can-and-can-t-do-for-intelligence"><em>What ChatGPT Can and Can&#8217;t Do for Intelligence</em></a>. Stephen Coulthart, Sam Keller, and Michael Young explore uses for large language models like ChatGPT in intelligence work.</li>
<li><a href="https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/"><em>PoisonGPT: How we Hid a Lobotomized LLM on HuggingFace to Spread Fake News</em></a>. Researchers surgically modified a large language model and then distributed it in an interesting new supply chain attack vector.</li>
<li><a href="https://mwi.westpoint.edu/trust-the-ai-but-keep-your-powder-dry-a-framework-for-balance-and-confidence-in-human-machine-teams/"><em>Trust the AI, But Keep Your Powder Dry: A Framework for Balance and Confidence in Human-Machine Teams</em></a>. Thomas Gaines and Amanda Mercier discuss the application of principles for building human teams to building trust in human-machine hybrid teams.</li>
<li><a href="https://arxiv.org/abs/2407.03453"><em>On Large Language Models in National Security Applications</em></a>. William Caballero and Phillip Jenkins from the Air Force Institute of Technology discuss opportunities for artificial intelligence integration into the national security establishment.</li>
<li><a href="https://mwi.westpoint.edu/advantage-defense-artificial-intelligence-at-the-tactical-cyber-edge/"><em>Advantage Defense: Artificial Intelligence at the Tactical Cyber Edge</em></a>. I wrote this article for the Modern War Institute at West Point to highlight early opportunities for artificial intelligence in military cyber applications: to accelerate the development of machine learning models to deal with ever-increasing amounts of data; as an analyst support tool to accelerate the pace of analysis; to improve warning intelligence; and to create realistic training.</li>
<li><a href="https://warontherocks.com/2025/07/friction-fog-and-failure-in-a-software-defined-force/"><em>Friction, Fog, and Failure in a Software-Defined Force</em></a>. Anthony Quitugua explains the risks of a brittle, connected modern military. Although not explicitly related to artificial intelligence, this perspective ought to inform all discussions of applying artificial intelligence in the military.</li>
<li><a href="https://www.anthropic.com/research/small-samples-poison"><em>A small number of samples can poison LLMs of any size</em></a>. Researchers at Anthropic, the UK&#8217;s AI Safety Institute, and the Alan Turing Institute demonstrate that as few as 250 documents can poison large language models. As AI becomes more integrated in military use cases, this will become an area of increasing importance.</li>
</ul>

<h3 class="headers" id="AdditionalResources">Additional Resources<span>&nbsp;<a href="#AdditionalResources">#</a></span></h3>

<p>This section lists a few useful tools and resources not specifically related to the previous sections.</p>

<ul>
    <li><a href="https://lmarena.ai/?leaderboard">Chatbot Arena</a>. This site helps users compare language models and posts community-sourced rankings.</li>
<li><a href="https://epoch.ai/data/large-scale-ai-models">Large-Scale AI Models</a>. Epoch AI tracks large-scale model creation. This is an interesting way to conpare different models. See also their report, <a href="https://epoch.ai/blog/tracking-large-scale-ai-models"><em>Tracking Large-Scale Models</em></a>.</li>
<li><a href="https://apps.apple.com/us/app/mlc-chat/id6448482937">MLC Chat</a>. For Apple devices like iPhones and iPads, this app makes it easy to run small langage models locally and offline.</li>
<li><a href="https://llm.university/">LLM University</a>. A nice collection of videos and text-based explanations of large language models and the underlying technologies.</li>
<li><a href="https://theaidigest.org/progress-and-dangers">How fast is AI improving?</a>. An interactive website that demonstrates how large language models have increased in capability over the years&#160;&#8212;&#160;and the associated dangers.</li>
<li><a href="https://bbycroft.net/llm">LLM Visualization</a>. Brendan Bycroft created an informative, interactive guide to understanding large language models. The website walks through the entire inference process both visually and by explanation.</li>
<li><a href="https://www.bulletpapers.ai/">Bullet Papers</a> and <a href="https://papers.day/">Papers.day</a> both provide artificial intelligence-generated summaries of ArXiv papers.</li>
<li><a href="https://www.promptingguide.ai/">Prompt Engineering Guide</a>. Also check out OpenAI&#8217;s <a href="https://platform.openai.com/docs/guides/prompt-engineering">Prompt engineering</a> documentation, and Meta&#8217;s <a href="https://www.llama.com/docs/how-to-guides/prompting/">Prompting</a> guide.</li>

</ul>

</article>
<p>
<a href="/blog/artificial-intelligence-reading-list.html">Permalink.</a>
</p>

        </main>
        <footer>
            <p>
                Follow me on <a href="http://twitter.com/zacjszewczyk">Twitter</a>, <a href="https://www.instagram.com/zacjszewczyk/">Instagram</a>, <a href="https://www.linkedin.com/in/zachary-szewczyk-441969235/">LinkedIn</a>, or subscribe to my <a href="/rss.xml">RSS</a> feed.
            </p>
            <p>
                © 2012-2025 Zachary Szewczyk.
            </p>
            <p>
                This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. See <a href="/disclaimers.html">disclaimers</a>.
            </p>
        </footer>
        <div id="lg"></div>
        <div id="rg"></div>
    </body>
    <link rel="manifest" href="/assets/manifest.json">
</html>